{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 프로젝트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy startproject naver_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naver_article/naver_article/items.py\n",
    "import scrapy\n",
    "\n",
    "class NaverArticleItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    press = scrapy.Field()\n",
    "    content = scrapy.Field()\n",
    "    category = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    photo_url = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_article/naver_article/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_article/naver_article/spiders/spider.py\n",
    "import scrapy\n",
    "import datetime\n",
    "from naver_article.items import NaverArticleItem\n",
    "from selenium import webdriver\n",
    "\n",
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = 'NaverArticle'\n",
    "    \n",
    "    def __init__(self, keyword=\"자살\", start_date=\"2015.01.01\", end_date=\"2018.12.31\", **kwargs):\n",
    "        self.start_urls = \"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}\".format(keyword)\n",
    "        self.search_press_ls = [\n",
    "            # 종합지\n",
    "            \"경향신문\", \"국민일보\", \"동아일보\", \"문화일보\", \"서울신문\", \"세계일보\", \"조선일보\", \"중앙일보\", \"한겨레\", \"한국일보\",\n",
    "            # 방송/통신사\n",
    "            \"JTBC\", \"KBS\", \"MBC\", \"MBN\", \"SBS CNBC\", \"SBS\", \"TV조선\",\"YTN\",\n",
    "            \"뉴스1\", \"뉴시스\", \"연합뉴스\", \"연합뉴스TV\", \"채널A\", \"한국경제TV\",\n",
    "            # 경제지\n",
    "            \"매일경제\", \"머니투데이\", \"서울경제\", \"아시아경제\", \"이데일리\", \"조선비즈\",\n",
    "            \"조세일보\", \"파이낸셜뉴스\", \"한국경제\", \"헤럴드경제\",\n",
    "            # 인터넷/IT지\n",
    "            \"ZDNet Korea\", \"노컷뉴스\", \"데일리안\", \"디지털데일리\", \"디지털타임스\", \"머니S\",\n",
    "            \"미디어오늘\", \"블로터\", \"아이뉴스24\", \"오마이뉴스\", \"전자신문\", \"프레시안\",\n",
    "            # 매거진\n",
    "            \"뉴스위크\", \"매경이코노미\", \"시사IN\", \"시사저널\", \"신동아\", \"월간 산\", \"이코노미스트\", \n",
    "            \"주간경향\", \"주간동아\", \"주간조선\", \"중앙SUNDAY\", \"한겨레21\", \"한경비즈니스\",\n",
    "            # 전문지/포토\n",
    "            \"기자협회보\", \"뉴스타파\", \"동아사이언스\", \"여성신문\", \"일다\", \"참세상\", \n",
    "            \"코리아헤럴드\", \"코메디닷컴\", \"헬스조선\",\n",
    "        ]\n",
    "        self.start_date = datetime.datetime.strptime(start_date, \"%Y.%m.%d\")\n",
    "        self.end_date = datetime.datetime.strptime(end_date, \"%Y.%m.%d\")\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    ## 사용하는 함수들\n",
    "    \n",
    "    ## 네이버 뉴스 신문사별 id 가져오기: {\"경향신문\" : \"1032\"}\n",
    "    def get_press_dict(self, driver):\n",
    "        driver.find_element_by_xpath('//*[@id=\"news_popup\"]').click()\n",
    "        keys = [element.get_attribute(\"title\") for element in driver.find_elements_by_css_selector('#order_cat li label')]\n",
    "        values = [element.get_attribute(\"value\") for element in driver.find_elements_by_css_selector('#order_cat li input')]\n",
    "        driver.find_element_by_xpath('//*[@id=\"news_popup\"]').click()\n",
    "        return {key:value for key, value in zip(keys, values)}\n",
    "    \n",
    "    ## 네이버 뉴스 검색할 신문사 선택하기\n",
    "    def set_search_press(self, driver):\n",
    "        press_dict = self.get_press_dict(driver)\n",
    "        driver.find_element_by_xpath('//*[@id=\"news_popup\"]').click()\n",
    "\n",
    "        ## 선택되어있는거 초기화\n",
    "        categorys = driver.find_elements_by_css_selector('#order_cat .viewtit input')\n",
    "        for category in categorys:\n",
    "            if category.get_attribute(\"checked\") != \"true\":\n",
    "                category.click()\n",
    "            category.click()\n",
    "\n",
    "        ## 검색할 신문사 선택\n",
    "        for press in self.search_press_ls:\n",
    "            driver.find_element_by_xpath('//*[@id=\"ca_{}\"]'.format(press_dict[press])).click()\n",
    "        driver.find_element_by_xpath('//*[@id=\"_nx_option_media\"]/div[2]/div[3]/button[1]').click()\n",
    "        \n",
    "    ## 네이버 뉴스 검색 기간 설정하기(왜이렇게 입력이 안됨...)\n",
    "    def set_search_date(self, driver, start_date, end_date):\n",
    "        driver.find_element_by_xpath('//*[@id=\"snb\"]/div/ul/li[2]').click()\n",
    "        while True:\n",
    "            driver.find_element_by_xpath('//*[@id=\"news_input_period_begin\"]').clear()\n",
    "            driver.find_element_by_xpath('//*[@id=\"news_input_period_begin\"]').send_keys(start_date)\n",
    "            input_start_date = driver.find_element_by_xpath('//*[@id=\"news_input_period_begin\"]').get_attribute(\"value\")\n",
    "            if start_date == input_start_date:\n",
    "                break\n",
    "        while True:\n",
    "            driver.find_element_by_xpath('//*[@id=\"news_input_period_end\"]').clear()\n",
    "            driver.find_element_by_xpath('//*[@id=\"news_input_period_end\"]').send_keys(end_date)        \n",
    "            input_end_date = driver.find_element_by_xpath('//*[@id=\"news_input_period_end\"]').get_attribute(\"value\")\n",
    "            if end_date == input_end_date:\n",
    "                break\n",
    "        driver.find_element_by_xpath('//*[@id=\"_nx_option_date\"]/div[2]/span/button').click()\n",
    "        \n",
    "    ## 텍스트 다듬어 넣기\n",
    "    def content_clear(self, content):\n",
    "        return content.replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\"\\xa0\", \"\").strip()\n",
    "    \n",
    "    \n",
    "    ## 스파이더 시작\n",
    "    \n",
    "    def start_requests(self):\n",
    "        url = self.start_urls\n",
    "        yield scrapy.Request(url, callback=self.parse) \n",
    "    \n",
    "    def parse(self, response):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('headless')\n",
    "        options.add_argument('window-size=1920x1080')\n",
    "        options.add_argument(\"disable-gpu\")\n",
    "        options.add_argument(\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\")\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(response.url)\n",
    "        driver.find_element_by_xpath('//*[@id=\"_search_option_btn\"]').click()\n",
    "        driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div/div[1]/div[3]/ul/li[2]/a').click()\n",
    "        self.set_search_press(driver)\n",
    "        \n",
    "        temp_start_date = self.start_date\n",
    "        temp_end_date = temp_start_date + datetime.timedelta(days=9)\n",
    "        \n",
    "        while True:\n",
    "            if temp_end_date >= self.end_date:\n",
    "                temp_end_date = self.end_date\n",
    "            \n",
    "            self.set_search_date(driver, temp_start_date.strftime(\"%Y.%m.%d\"), temp_end_date.strftime(\"%Y.%m.%d\"))\n",
    "            try:\n",
    "                while True:\n",
    "                    links = [element.get_attribute(\"href\") for element in driver.find_elements_by_xpath('//*[@id=\"main_pack\"]/div/ul/li/dl/dd/a')]\n",
    "                    for link in links:\n",
    "                        yield scrapy.Request(link, callback=self.parse_page_contents, dont_filter=True)\n",
    "                    ## 다음페이지 클릭\n",
    "                    driver.find_element_by_css_selector('#main_pack > div > div.paging > a.next').click()\n",
    "            ## 다음 페이지가 없으면 에러남\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if temp_end_date == self.end_date:\n",
    "                driver.quit()\n",
    "                break\n",
    "            temp_start_date += datetime.timedelta(days=10)\n",
    "            temp_end_date += datetime.timedelta(days=10)\n",
    "            \n",
    "    def parse_page_contents(self, response):\n",
    "        if \"entertain\" in response.url:\n",
    "            title = self.content_clear(response.xpath('//*[@id=\"content\"]/div[1]/div/h2/text()').extract()[0])\n",
    "            category = \"TV연예\"\n",
    "            press = response.xpath('//*[@id=\"content\"]/div[1]/div/div[1]/a/img/@alt').extract()[0]\n",
    "            try:\n",
    "                date = response.xpath('//*[@id=\"main_content\"]/div[1]/div[3]/div/span/text()').extract()[0].replace(\"오전\", \"AM\").replace(\"오후\", \"PM\")\n",
    "                date = datetime.datetime.strptime(date, \"%Y.%m.%d. %p %I:%M\")\n",
    "            except:\n",
    "                date = response.xpath('//*[@id=\"content\"]/div[1]/div/div[2]/span/em/text()').extract()[0].replace(\"오전\", \"AM\").replace(\"오후\", \"PM\")\n",
    "                date = datetime.datetime.strptime(date, \"%Y.%m.%d. %p %I:%M\")\n",
    "            content = [cont.strip() for cont in response.xpath('//*[@id=\"articeBody\"]/text()').extract()]\n",
    "            content = self.content_clear(\" \".join(content))\n",
    "            \n",
    "            photo_url = response.xpath('//*[@class=\"end_photo_org\"]/img/@src').extract()\n",
    "\n",
    "\n",
    "        else:\n",
    "            title = self.content_clear(response.xpath('//*[@id=\"articleTitle\"]/text()').extract()[0])\n",
    "            try:\n",
    "                category = response.xpath('//*[@id=\"articleBody\"]/div[2]/a/em/text()').extract()[0]\n",
    "            except:\n",
    "                category = \"-\"\n",
    "            press = response.xpath('//*[@id=\"main_content\"]/div[1]/div[1]/a/img/@title').extract()[0]\n",
    "            try:\n",
    "                date = response.xpath('//*[@id=\"main_content\"]/div[1]/div[3]/div/span/text()').extract()[0].replace(\"오전\", \"AM\").replace(\"오후\", \"PM\")\n",
    "                date = datetime.datetime.strptime(date, \"%Y.%m.%d. %p %I:%M\")\n",
    "            except:\n",
    "                date = response.xpath('//*[@id=\"main_content\"]/div[1]/div[3]/div/span[2]/text()').extract()[0].replace(\"오전\", \"AM\").replace(\"오후\", \"PM\")\n",
    "                date = datetime.datetime.strptime(date, \"%Y.%m.%d. %p %I:%M\")\n",
    "            content = [cont.strip() for cont in response.xpath('//*[@id=\"articleBodyContents\"]/text()').extract()]\n",
    "            content = self.content_clear(\" \".join(content))\n",
    "            photo_url = response.xpath('//*[@class=\"end_photo_org\"]/img/@src').extract()\n",
    "        \n",
    "        item = NaverArticleItem()\n",
    "        item[\"title\"] = title\n",
    "        item[\"link\"] = response.url\n",
    "        item[\"category\"] = category\n",
    "        item[\"press\"] = press\n",
    "        item[\"date\"] = date\n",
    "        item[\"content\"] = content\n",
    "        item[\"photo_url\"] = photo_url\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### robots.txt 설정 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i .bak 's/ROBOTSTXT_OBEY = True/ROBOTSTXT_OBEY = False/' naver_article/naver_article/settings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실행 파일 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "cd naver_article/\n",
    "scrapy crawl NaverArticle -o naver_article.csv -a keyword=\"자살\" -a start_date=\"2015.01.01\" -a end_date=\"2018.12.31\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source run.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>photo_url</th>\n",
       "      <th>press</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120374</th>\n",
       "      <td>생활</td>\n",
       "      <td>[오마이뉴스 오창균 기자] 어릴 때의 기억이다. 트랜지스터 라디오에서 흘러나오던 목...</td>\n",
       "      <td>2015-01-10 14:24:00</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>오마이뉴스</td>\n",
       "      <td>어릴 적 추억 담긴 '씨아기'와 '문래'... 뭔지 아시나요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120375</th>\n",
       "      <td>TV연예</td>\n",
       "      <td>‘하트투하트’ 최강희가 천정명과 강렬한 첫 만남을 가졌다.  9일 첫 방송한 tvN...</td>\n",
       "      <td>2015-01-09 23:56:00</td>\n",
       "      <td>https://entertain.naver.com/read?oid=014&amp;aid=0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>파이낸셜뉴스</td>\n",
       "      <td>‘하트투하트’ 최강희, 사람 찌르는 천정명 목격? ‘스릴넘치는 첫만남’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120376</th>\n",
       "      <td>사회</td>\n",
       "      <td>내 인생의 첫 나무는 낙엽송이었다. 나무심기가 한창이던 1960년대, 초록색 싹눈이...</td>\n",
       "      <td>2015-01-10 14:10:00</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>조선비즈</td>\n",
       "      <td>[인터뷰] 30년 나무와 숲이 가르쳐 준 지혜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120377</th>\n",
       "      <td>TV연예</td>\n",
       "      <td>‘하트투하트’ 천정명이 첫 회부터 강렬한 시작을 알렸다.  지난 9일 첫 방송된 t...</td>\n",
       "      <td>2015-01-10 10:48:00</td>\n",
       "      <td>https://entertain.naver.com/read?oid=014&amp;aid=0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>파이낸셜뉴스</td>\n",
       "      <td>‘하트투하트’ 천정명, 뻔뻔하고 능청스러운 정신과 의사로 ‘완벽빙의’</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120378</th>\n",
       "      <td>TV연예</td>\n",
       "      <td>[이데일리 스타in 강민정 기자] 케이블챈ㄹ tvN 금토드라마 ‘하트투하트’가 시청...</td>\n",
       "      <td>2015-01-10 08:57:00</td>\n",
       "      <td>https://entertain.naver.com/read?oid=018&amp;aid=0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>이데일리</td>\n",
       "      <td>최강희표 로코 '하트투하트', 기분 좋은 출발..1040女 꽉 잡았다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                            content  \\\n",
       "120374       생활  [오마이뉴스 오창균 기자] 어릴 때의 기억이다. 트랜지스터 라디오에서 흘러나오던 목...   \n",
       "120375     TV연예  ‘하트투하트’ 최강희가 천정명과 강렬한 첫 만남을 가졌다.  9일 첫 방송한 tvN...   \n",
       "120376       사회  내 인생의 첫 나무는 낙엽송이었다. 나무심기가 한창이던 1960년대, 초록색 싹눈이...   \n",
       "120377     TV연예  ‘하트투하트’ 천정명이 첫 회부터 강렬한 시작을 알렸다.  지난 9일 첫 방송된 t...   \n",
       "120378     TV연예  [이데일리 스타in 강민정 기자] 케이블챈ㄹ tvN 금토드라마 ‘하트투하트’가 시청...   \n",
       "\n",
       "                       date  \\\n",
       "120374  2015-01-10 14:24:00   \n",
       "120375  2015-01-09 23:56:00   \n",
       "120376  2015-01-10 14:10:00   \n",
       "120377  2015-01-10 10:48:00   \n",
       "120378  2015-01-10 08:57:00   \n",
       "\n",
       "                                                     link photo_url   press  \\\n",
       "120374  https://news.naver.com/main/read.nhn?mode=LSD&...       NaN   오마이뉴스   \n",
       "120375  https://entertain.naver.com/read?oid=014&aid=0...       NaN  파이낸셜뉴스   \n",
       "120376  https://news.naver.com/main/read.nhn?mode=LSD&...       NaN    조선비즈   \n",
       "120377  https://entertain.naver.com/read?oid=014&aid=0...       NaN  파이낸셜뉴스   \n",
       "120378  https://entertain.naver.com/read?oid=018&aid=0...       NaN    이데일리   \n",
       "\n",
       "                                          title  \n",
       "120374        어릴 적 추억 담긴 '씨아기'와 '문래'... 뭔지 아시나요  \n",
       "120375  ‘하트투하트’ 최강희, 사람 찌르는 천정명 목격? ‘스릴넘치는 첫만남’  \n",
       "120376                [인터뷰] 30년 나무와 숲이 가르쳐 준 지혜  \n",
       "120377   ‘하트투하트’ 천정명, 뻔뻔하고 능청스러운 정신과 의사로 ‘완벽빙의’  \n",
       "120378   최강희표 로코 '하트투하트', 기분 좋은 출발..1040女 꽉 잡았다  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"naver_article/naver_article.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = df.date.astype(\"datetime64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.date.apply(lambda x : str(x.year) + \"-\" + str(x.month) + \"-\" + str(x.day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1461"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1460"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "365*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112369"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.content.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
