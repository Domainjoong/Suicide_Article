{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 프로젝트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy startproject naver_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naver_article/naver_article/items.py\n",
    "import scrapy\n",
    "\n",
    "class NaverArticleItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    press = scrapy.Field()\n",
    "    content = scrapy.Field()\n",
    "    category = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    photo_url = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_article/naver_article/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_article/naver_article/spiders/spider.py\n",
    "import scrapy\n",
    "import datetime\n",
    "from naver_article.items import NaverArticleItem\n",
    "from selenium import webdriver\n",
    "\n",
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = 'NaverArticle'\n",
    "    \n",
    "    def __init__(self, keyword=\"자살\", start_date=\"2015.01.01\", end_date=\"2018.12.31\", **kwargs):\n",
    "        self.start_urls = \"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}\".format(keyword)\n",
    "        self.search_press_ls = [\n",
    "            # 종합지\n",
    "            \"경향신문\", \"국민일보\", \"동아일보\", \"문화일보\", \"서울신문\", \"세계일보\", \"조선일보\", \"중앙일보\", \"한겨레\", \"한국일보\",\n",
    "            # 방송/통신사\n",
    "            \"JTBC\", \"KBS\", \"MBC\", \"MBN\", \"SBS CNBC\", \"SBS\", \"TV조선\",\"YTN\",\n",
    "            \"뉴스1\", \"뉴시스\", \"연합뉴스\", \"연합뉴스TV\", \"채널A\", \"한국경제TV\",\n",
    "            # 경제지\n",
    "            \"매일경제\", \"머니투데이\", \"서울경제\", \"아시아경제\", \"이데일리\", \"조선비즈\",\n",
    "            \"조세일보\", \"파이낸셜뉴스\", \"한국경제\", \"헤럴드경제\",\n",
    "            # 인터넷/IT지\n",
    "            \"ZDNet Korea\", \"노컷뉴스\", \"데일리안\", \"디지털데일리\", \"디지털타임스\", \"머니S\",\n",
    "            \"미디어오늘\", \"블로터\", \"아이뉴스24\", \"오마이뉴스\", \"전자신문\", \"프레시안\",\n",
    "            # 매거진\n",
    "            \"뉴스위크\", \"매경이코노미\", \"시사IN\", \"시사저널\", \"신동아\", \"월간 산\", \"이코노미스트\", \n",
    "            \"주간경향\", \"주간동아\", \"주간조선\", \"중앙SUNDAY\", \"한겨레21\", \"한경비즈니스\",\n",
    "            # 전문지/포토\n",
    "            \"기자협회보\", \"뉴스타파\", \"동아사이언스\", \"여성신문\", \"일다\", \"참세상\", \n",
    "            \"코리아헤럴드\", \"코메디닷컴\", \"헬스조선\",\n",
    "        ]\n",
    "        self.start_date = datetime.datetime.strptime(start_date, \"%Y.%m.%d\")\n",
    "        self.end_date = datetime.datetime.strptime(end_date, \"%Y.%m.%d\")\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    ## 사용하는 함수들\n",
    "    \n",
    "    ## 네이버 뉴스 신문사별 id 가져오기: {\"경향신문\" : \"1032\"}\n",
    "    def get_press_dict(self, driver):\n",
    "        driver.find_element_by_xpath('//*[@id=\"news_popup\"]').click()\n",
    "        keys = [element.get_attribute(\"title\") for element in driver.find_elements_by_css_selector('#order_cat li label')]\n",
    "        values = [element.get_attribute(\"value\") for element in driver.find_elements_by_css_selector('#order_cat li input')]\n",
    "        driver.find_element_by_xpath('//*[@id=\"news_popup\"]').click()\n",
    "        return {key:value for key, value in zip(keys, values)}\n",
    "    \n",
    "    ## 네이버 뉴스 검색할 신문사 선택하기\n",
    "    def set_search_press(self, driver):\n",
    "        press_dict = self.get_press_dict(driver)\n",
    "        driver.find_element_by_xpath('//*[@id=\"news_popup\"]').click()\n",
    "\n",
    "        ## 선택되어있는거 초기화\n",
    "        categorys = driver.find_elements_by_css_selector('#order_cat .viewtit input')\n",
    "        for category in categorys:\n",
    "            if category.get_attribute(\"checked\") != \"true\":\n",
    "                category.click()\n",
    "            category.click()\n",
    "\n",
    "        ## 검색할 신문사 선택\n",
    "        for press in self.search_press_ls:\n",
    "            driver.find_element_by_xpath('//*[@id=\"ca_{}\"]'.format(press_dict[press])).click()\n",
    "        driver.find_element_by_xpath('//*[@id=\"_nx_option_media\"]/div[2]/div[3]/button[1]').click()\n",
    "        \n",
    "    ## 네이버 뉴스 검색 기간 설정하기(왜이렇게 입력이 안됨...)\n",
    "    def set_search_date(self, driver, start_date, end_date):\n",
    "        driver.find_element_by_xpath('//*[@id=\"snb\"]/div/ul/li[2]').click()\n",
    "        while True:\n",
    "            driver.find_element_by_xpath('//*[@id=\"news_input_period_begin\"]').clear()\n",
    "            driver.find_element_by_xpath('//*[@id=\"news_input_period_begin\"]').send_keys(start_date)\n",
    "            input_start_date = driver.find_element_by_xpath('//*[@id=\"news_input_period_begin\"]').get_attribute(\"value\")\n",
    "            if start_date == input_start_date:\n",
    "                break\n",
    "        while True:\n",
    "            driver.find_element_by_xpath('//*[@id=\"news_input_period_end\"]').clear()\n",
    "            driver.find_element_by_xpath('//*[@id=\"news_input_period_end\"]').send_keys(end_date)        \n",
    "            input_end_date = driver.find_element_by_xpath('//*[@id=\"news_input_period_end\"]').get_attribute(\"value\")\n",
    "            if end_date == input_end_date:\n",
    "                break\n",
    "        driver.find_element_by_xpath('//*[@id=\"_nx_option_date\"]/div[2]/span/button').click()\n",
    "        \n",
    "    ## 텍스트 다듬어 넣기\n",
    "    def content_clear(self, content):\n",
    "        return content.replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\"\\xa0\", \"\").strip()\n",
    "    \n",
    "    \n",
    "    ## 스파이더 시작\n",
    "    \n",
    "    def start_requests(self):\n",
    "        url = self.start_urls\n",
    "        yield scrapy.Request(url, callback=self.parse) \n",
    "    \n",
    "    def parse(self, response):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('headless')\n",
    "        options.add_argument('window-size=1920x1080')\n",
    "        options.add_argument(\"disable-gpu\")\n",
    "        options.add_argument(\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\")\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(response.url)\n",
    "        driver.find_element_by_xpath('//*[@id=\"_search_option_btn\"]').click()\n",
    "        driver.find_element_by_xpath('//*[@id=\"main_pack\"]/div/div[1]/div[3]/ul/li[2]/a').click()\n",
    "        self.set_search_press(driver)\n",
    "        \n",
    "        temp_start_date = self.start_date\n",
    "        temp_end_date = temp_start_date + datetime.timedelta(days=9)\n",
    "        \n",
    "        while True:\n",
    "            if temp_end_date >= self.end_date:\n",
    "                temp_end_date = self.end_date\n",
    "            \n",
    "            self.set_search_date(driver, temp_start_date.strftime(\"%Y.%m.%d\"), temp_end_date.strftime(\"%Y.%m.%d\"))\n",
    "            try:\n",
    "                while True:\n",
    "                    links = [element.get_attribute(\"href\") for element in driver.find_elements_by_xpath('//*[@id=\"main_pack\"]/div/ul/li/dl/dd/a')]\n",
    "                    for link in links:\n",
    "                        yield scrapy.Request(link, callback=self.parse_page_contents, dont_filter=True)\n",
    "                    ## 다음페이지 클릭\n",
    "                    driver.find_element_by_css_selector('#main_pack > div > div.paging > a.next').click()\n",
    "            ## 다음 페이지가 없으면 에러남\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if temp_end_date == self.end_date:\n",
    "                driver.quit()\n",
    "                break\n",
    "            temp_start_date += datetime.timedelta(days=10)\n",
    "            temp_end_date += datetime.timedelta(days=10)\n",
    "            \n",
    "    def parse_page_contents(self, response):\n",
    "        if \"entertain\" in response.url:\n",
    "            title = self.content_clear(response.xpath('//*[@id=\"content\"]/div[1]/div/h2/text()').extract()[0])\n",
    "            category = \"TV연예\"\n",
    "            press = response.xpath('//*[@id=\"content\"]/div[1]/div/div[1]/a/img/@alt').extract()[0]\n",
    "            try:\n",
    "                date = response.xpath('//*[@id=\"main_content\"]/div[1]/div[3]/div/span/text()').extract()[0].replace(\"오전\", \"AM\").replace(\"오후\", \"PM\")\n",
    "                date = datetime.datetime.strptime(date, \"%Y.%m.%d. %p %I:%M\")\n",
    "            except:\n",
    "                date = response.xpath('//*[@id=\"content\"]/div[1]/div/div[2]/span/em/text()').extract()[0].replace(\"오전\", \"AM\").replace(\"오후\", \"PM\")\n",
    "                date = datetime.datetime.strptime(date, \"%Y.%m.%d. %p %I:%M\")\n",
    "            content = [cont.strip() for cont in response.xpath('//*[@id=\"articeBody\"]/text()').extract()]\n",
    "            content = self.content_clear(\" \".join(content))\n",
    "            \n",
    "            photo_url = response.xpath('//*[@class=\"end_photo_org\"]/img/@src').extract()\n",
    "\n",
    "\n",
    "        else:\n",
    "            title = self.content_clear(response.xpath('//*[@id=\"articleTitle\"]/text()').extract()[0])\n",
    "            try:\n",
    "                category = response.xpath('//*[@id=\"articleBody\"]/div[2]/a/em/text()').extract()[0]\n",
    "            except:\n",
    "                category = \"-\"\n",
    "            press = response.xpath('//*[@id=\"main_content\"]/div[1]/div[1]/a/img/@title').extract()[0]\n",
    "            try:\n",
    "                date = response.xpath('//*[@id=\"main_content\"]/div[1]/div[3]/div/span/text()').extract()[0].replace(\"오전\", \"AM\").replace(\"오후\", \"PM\")\n",
    "                date = datetime.datetime.strptime(date, \"%Y.%m.%d. %p %I:%M\")\n",
    "            except:\n",
    "                date = response.xpath('//*[@id=\"main_content\"]/div[1]/div[3]/div/span[2]/text()').extract()[0].replace(\"오전\", \"AM\").replace(\"오후\", \"PM\")\n",
    "                date = datetime.datetime.strptime(date, \"%Y.%m.%d. %p %I:%M\")\n",
    "            content = [cont.strip() for cont in response.xpath('//*[@id=\"articleBodyContents\"]/text()').extract()]\n",
    "            content = self.content_clear(\" \".join(content))\n",
    "            photo_url = response.xpath('//*[@class=\"end_photo_org\"]/img/@src').extract()\n",
    "        \n",
    "        item = NaverArticleItem()\n",
    "        item[\"title\"] = title\n",
    "        item[\"link\"] = response.url\n",
    "        item[\"category\"] = category\n",
    "        item[\"press\"] = press\n",
    "        item[\"date\"] = date\n",
    "        item[\"content\"] = content\n",
    "        item[\"photo_url\"] = photo_url\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### robots.txt 설정 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i .bak 's/ROBOTSTXT_OBEY = True/ROBOTSTXT_OBEY = False/' naver_article/naver_article/settings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실행 파일 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "cd naver_article/\n",
    "scrapy crawl NaverArticle -o naver_article.csv -a keyword=\"자살\" -a start_date=\"2008.01.01\" -a end_date=\"2014.12.31\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source run.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>photo_url</th>\n",
       "      <th>press</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170806</th>\n",
       "      <td>TV연예</td>\n",
       "      <td>온미디어 계열의 OCNTV무비 '에로틱판타지 천일야화'에서 타로 카페 주인 김보경이...</td>\n",
       "      <td>2008-01-10 11:42:00</td>\n",
       "      <td>https://entertain.naver.com/read?oid=015&amp;aid=0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>한국경제</td>\n",
       "      <td>'에로틱 판타지 천일야화', 와인 이벤트 실시</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170807</th>\n",
       "      <td>TV연예</td>\n",
       "      <td>무기력하게 살고 있는 여고생 와라(이시하라 사토미). 손목을 다쳐 찾아간 병원에서 ...</td>\n",
       "      <td>2008-01-10 14:00:00</td>\n",
       "      <td>https://entertain.naver.com/read?oid=021&amp;aid=0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>문화일보</td>\n",
       "      <td>청춘들의 상처 ? 붕대 한 번 감았다 풀면 낫는 것!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170808</th>\n",
       "      <td>-</td>\n",
       "      <td>근 20년 만에 다시 미국에 온 것은 오로지 인디언을 다시 찾기 위해서다. 그러나 ...</td>\n",
       "      <td>2008-01-10 18:13:00</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LSD&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>경향신문</td>\n",
       "      <td>[박홍규칼럼]인디언 마을에서</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170809</th>\n",
       "      <td>TV연예</td>\n",
       "      <td>“장금이에게 인내와 긍정의 힘을 배웠습니다.” 한류 열풍이 인도, 이란 등 제3세계...</td>\n",
       "      <td>2008-01-10 13:01:00</td>\n",
       "      <td>https://entertain.naver.com/read?oid=016&amp;aid=0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>헤럴드경제</td>\n",
       "      <td>韓流 이젠 ‘제3세계로’ 흐른다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170810</th>\n",
       "      <td>TV연예</td>\n",
       "      <td>공포 마니아들과 미국 드라마 마니아들이 환호성을 올릴 마니아 시간이 마련된다. 영화...</td>\n",
       "      <td>2008-01-10 14:30:00</td>\n",
       "      <td>https://entertain.naver.com/read?oid=021&amp;aid=0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>문화일보</td>\n",
       "      <td>마니아들 ‘케이블 채널로 모여라’</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                            content  \\\n",
       "170806     TV연예  온미디어 계열의 OCNTV무비 '에로틱판타지 천일야화'에서 타로 카페 주인 김보경이...   \n",
       "170807     TV연예  무기력하게 살고 있는 여고생 와라(이시하라 사토미). 손목을 다쳐 찾아간 병원에서 ...   \n",
       "170808        -  근 20년 만에 다시 미국에 온 것은 오로지 인디언을 다시 찾기 위해서다. 그러나 ...   \n",
       "170809     TV연예  “장금이에게 인내와 긍정의 힘을 배웠습니다.” 한류 열풍이 인도, 이란 등 제3세계...   \n",
       "170810     TV연예  공포 마니아들과 미국 드라마 마니아들이 환호성을 올릴 마니아 시간이 마련된다. 영화...   \n",
       "\n",
       "                       date  \\\n",
       "170806  2008-01-10 11:42:00   \n",
       "170807  2008-01-10 14:00:00   \n",
       "170808  2008-01-10 18:13:00   \n",
       "170809  2008-01-10 13:01:00   \n",
       "170810  2008-01-10 14:30:00   \n",
       "\n",
       "                                                     link photo_url  press  \\\n",
       "170806  https://entertain.naver.com/read?oid=015&aid=0...       NaN   한국경제   \n",
       "170807  https://entertain.naver.com/read?oid=021&aid=0...       NaN   문화일보   \n",
       "170808  https://news.naver.com/main/read.nhn?mode=LSD&...       NaN   경향신문   \n",
       "170809  https://entertain.naver.com/read?oid=016&aid=0...       NaN  헤럴드경제   \n",
       "170810  https://entertain.naver.com/read?oid=021&aid=0...       NaN   문화일보   \n",
       "\n",
       "                                title  \n",
       "170806      '에로틱 판타지 천일야화', 와인 이벤트 실시  \n",
       "170807  청춘들의 상처 ? 붕대 한 번 감았다 풀면 낫는 것!  \n",
       "170808                [박홍규칼럼]인디언 마을에서  \n",
       "170809              韓流 이젠 ‘제3세계로’ 흐른다  \n",
       "170810             마니아들 ‘케이블 채널로 모여라’  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"naver_article/naver_article.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 기사 제거 이전: 170811개\n",
      "중복 기사 제거 이후: 151007개\n"
     ]
    }
   ],
   "source": [
    "## 중복 기사 제거\n",
    "print(\"중복 기사 제거 이전: {}개\".format(len(df)))\n",
    "df = df.drop_duplicates(['content'], keep='first')\n",
    "print(\"중복 기사 제거 이후: {}개\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
